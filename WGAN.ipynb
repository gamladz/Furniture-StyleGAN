{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "name": "WGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzG0Tg87W6m8"
      },
      "source": [
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import glob\n",
        "import os\n",
        "import platform\n",
        "import numpy as np\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "from ImageDataset import ImageDataset\n",
        "from WGAN_net import Generator, Discriminator, weights_init\n",
        "from WGAN_net import gradient_penalty\n",
        "\n",
        "%matplotlib inline\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yw9rjROhCDR"
      },
      "source": [
        "dataset = ImageDataset('DATA')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvOP-_2fs75_"
      },
      "source": [
        "# Decide which device we want to run on\n",
        "ngpu = 1\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "learning_rate = 1e-4\n",
        "batch_size = 64\n",
        "image_size = 64\n",
        "img_ch = 3\n",
        "workers = 2\n",
        "z_dim = 100\n",
        "num_epochs = 200\n",
        "features_g = 64\n",
        "features_d = 64\n",
        "critic_iterations = 5\n",
        "lambda_pen = 10"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTuU_oDOtCst"
      },
      "source": [
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=workers)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiTNUbUMt_Sf"
      },
      "source": [
        "# Create the Generator and Discriminator\n",
        "netG = Generator(z_dim=z_dim, img_ch=img_ch, features_g=features_g).to(device)\n",
        "netD = Discriminator(img_ch=img_ch, features_d=features_d).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
        "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "\n",
        "netD.apply(weights_init)\n",
        "netG.apply(weights_init)\n",
        "# Initialize BCELoss function\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=learning_rate, betas=(0.0, 0.9))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(0.0, 0.9))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNb-hz-c1Bqw",
        "outputId": "4e495c71-7c8d-4298-dbb2-c8a38d0afa35"
      },
      "source": [
        "fixed_noise = torch.randn(32, z_dim, 1, 1).to(device)\n",
        "\n",
        "# Number of training epochs\n",
        "writer_real = SummaryWriter(log_dir=f'logs2/real')\n",
        "writer_fake = SummaryWriter(log_dir=f'logs2/fake')\n",
        "writer_lossD = SummaryWriter(log_dir=f'logs2/lossD')\n",
        "writer_lossG = SummaryWriter(log_dir=f'logs2/lossG')\n",
        "writer_penalty = SummaryWriter(log_dir=f'logs2/penalty')\n",
        "iters = 0\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # For each batch in the dataloader\n",
        "    for i, (real, _) in enumerate(dataloader):\n",
        "        real = real.to(device)\n",
        "        for _ in range(critic_iterations):\n",
        "            noise = torch.randn(real.shape[0], z_dim, 1, 1).to(device)\n",
        "            fake = netG(noise)\n",
        "            disc_real = netD(real).reshape(-1)\n",
        "            disc_fake = netD(fake).reshape(-1)\n",
        "            gp = gradient_penalty(netD, real, fake, device=device)\n",
        "            errD_real = torch.mean(disc_real)\n",
        "            errD_fake = torch.mean(disc_fake)\n",
        "            loss_disc = (-(errD_real - errD_fake)\\\n",
        "                         + lambda_pen * gp)\n",
        "            netD.zero_grad()\n",
        "            loss_disc.backward(retain_graph=True)\n",
        "            optimizerD.step()\n",
        "\n",
        "        output = netD(fake).view(-1)\n",
        "        loss_gen = -torch.mean(output)\n",
        "        netG.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if i % 20 == 0:\n",
        "            print(\n",
        "                f'Epoch [{epoch}/{num_epochs}] Batch {i}/{len(dataloader)} '\n",
        "                + f'Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}'\n",
        "                )\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise)\n",
        "                img_grid_real = torchvision.utils.make_grid(\n",
        "                    real[:32], normalize=True\n",
        "                )\n",
        "                img_grid_fake = torchvision.utils.make_grid(\n",
        "                    fake[:32], normalize=True\n",
        "                )\n",
        "\n",
        "                writer_real.add_image('Real', img_grid_real, global_step=iters)\n",
        "                writer_real.add_scalar('Real', errD_real, global_step=iters)\n",
        "                writer_fake.add_image('D(x)', img_grid_fake, global_step=iters)\n",
        "                writer_fake.add_scalar('D(G(z))', errD_fake, global_step=iters)\n",
        "                writer_lossD.add_scalar('Loss_Discriminator', loss_disc.item(), global_step=iters)\n",
        "                writer_lossG.add_scalar('Loss_Generator', loss_gen.item(), global_step=iters)\n",
        "                writer_penalty.add_scalar('Gradient_Penalty', gp.item(), global_step=iters)\n",
        "            iters += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training Loop...\n",
            "Epoch [0/200] Batch 0/43 Loss D: -21.8954, loss G: 29.1958\n",
            "Epoch [0/200] Batch 20/43 Loss D: -21.3246, loss G: 42.1368\n",
            "Epoch [0/200] Batch 40/43 Loss D: -26.9997, loss G: 45.7437\n",
            "Epoch [1/200] Batch 0/43 Loss D: -31.9731, loss G: 44.3318\n",
            "Epoch [1/200] Batch 20/43 Loss D: -26.5479, loss G: 47.3191\n",
            "Epoch [1/200] Batch 40/43 Loss D: -24.7024, loss G: 47.7742\n",
            "Epoch [2/200] Batch 0/43 Loss D: -31.0710, loss G: 40.8739\n",
            "Epoch [2/200] Batch 20/43 Loss D: -27.5465, loss G: 49.6100\n",
            "Epoch [2/200] Batch 40/43 Loss D: -32.2657, loss G: 44.7909\n",
            "Epoch [3/200] Batch 0/43 Loss D: -27.3051, loss G: 52.6935\n",
            "Epoch [3/200] Batch 20/43 Loss D: -27.7235, loss G: 60.3729\n",
            "Epoch [3/200] Batch 40/43 Loss D: -30.3863, loss G: 49.7284\n",
            "Epoch [4/200] Batch 0/43 Loss D: -24.2956, loss G: 54.2500\n",
            "Epoch [4/200] Batch 20/43 Loss D: -27.5710, loss G: 56.5612\n",
            "Epoch [4/200] Batch 40/43 Loss D: -32.0501, loss G: 44.1032\n",
            "Epoch [5/200] Batch 0/43 Loss D: -25.2204, loss G: 54.3083\n",
            "Epoch [5/200] Batch 20/43 Loss D: -29.5482, loss G: 48.0302\n",
            "Epoch [5/200] Batch 40/43 Loss D: -28.0813, loss G: 60.3781\n",
            "Epoch [6/200] Batch 0/43 Loss D: -24.7496, loss G: 60.9721\n",
            "Epoch [6/200] Batch 20/43 Loss D: -27.4552, loss G: 69.9327\n",
            "Epoch [6/200] Batch 40/43 Loss D: -26.6095, loss G: 52.3182\n",
            "Epoch [7/200] Batch 0/43 Loss D: -26.9082, loss G: 67.3485\n",
            "Epoch [7/200] Batch 20/43 Loss D: -23.8608, loss G: 72.3340\n",
            "Epoch [7/200] Batch 40/43 Loss D: -24.7319, loss G: 54.0957\n",
            "Epoch [8/200] Batch 0/43 Loss D: -27.9831, loss G: 66.2621\n",
            "Epoch [8/200] Batch 20/43 Loss D: -26.1466, loss G: 59.3369\n",
            "Epoch [8/200] Batch 40/43 Loss D: -30.5181, loss G: 80.8006\n",
            "Epoch [9/200] Batch 0/43 Loss D: -24.3331, loss G: 70.4640\n",
            "Epoch [9/200] Batch 20/43 Loss D: -21.6382, loss G: 64.9353\n",
            "Epoch [9/200] Batch 40/43 Loss D: -28.2265, loss G: 74.0941\n",
            "Epoch [10/200] Batch 0/43 Loss D: -34.3605, loss G: 65.2019\n",
            "Epoch [10/200] Batch 20/43 Loss D: -25.2606, loss G: 60.7187\n",
            "Epoch [10/200] Batch 40/43 Loss D: -25.8814, loss G: 70.4722\n",
            "Epoch [11/200] Batch 0/43 Loss D: -22.2621, loss G: 63.7494\n",
            "Epoch [11/200] Batch 20/43 Loss D: -23.6309, loss G: 78.0038\n",
            "Epoch [11/200] Batch 40/43 Loss D: -25.3327, loss G: 72.6614\n",
            "Epoch [12/200] Batch 0/43 Loss D: -24.8349, loss G: 74.5939\n",
            "Epoch [12/200] Batch 20/43 Loss D: -27.6458, loss G: 85.1343\n",
            "Epoch [12/200] Batch 40/43 Loss D: -26.6078, loss G: 77.9107\n",
            "Epoch [13/200] Batch 0/43 Loss D: -28.1932, loss G: 85.3979\n",
            "Epoch [13/200] Batch 20/43 Loss D: -30.9691, loss G: 81.7007\n",
            "Epoch [13/200] Batch 40/43 Loss D: -30.5694, loss G: 79.7173\n",
            "Epoch [14/200] Batch 0/43 Loss D: -29.7201, loss G: 85.3190\n",
            "Epoch [14/200] Batch 20/43 Loss D: -29.5625, loss G: 87.1001\n",
            "Epoch [14/200] Batch 40/43 Loss D: -29.9810, loss G: 84.4444\n",
            "Epoch [15/200] Batch 0/43 Loss D: -32.6799, loss G: 103.3332\n",
            "Epoch [15/200] Batch 20/43 Loss D: -28.4335, loss G: 84.3570\n",
            "Epoch [15/200] Batch 40/43 Loss D: -23.1831, loss G: 81.3871\n",
            "Epoch [16/200] Batch 0/43 Loss D: -33.5406, loss G: 77.8355\n",
            "Epoch [16/200] Batch 20/43 Loss D: -25.4320, loss G: 78.8161\n",
            "Epoch [16/200] Batch 40/43 Loss D: -29.5039, loss G: 80.5706\n",
            "Epoch [17/200] Batch 0/43 Loss D: -27.0810, loss G: 86.1574\n",
            "Epoch [17/200] Batch 20/43 Loss D: -33.5021, loss G: 90.8039\n",
            "Epoch [17/200] Batch 40/43 Loss D: -27.1469, loss G: 93.3709\n",
            "Epoch [18/200] Batch 0/43 Loss D: -29.0256, loss G: 86.6161\n",
            "Epoch [18/200] Batch 20/43 Loss D: -33.8490, loss G: 97.3565\n",
            "Epoch [18/200] Batch 40/43 Loss D: -30.3912, loss G: 108.0843\n",
            "Epoch [19/200] Batch 0/43 Loss D: -29.7976, loss G: 80.9501\n",
            "Epoch [19/200] Batch 20/43 Loss D: -26.4443, loss G: 84.5665\n",
            "Epoch [19/200] Batch 40/43 Loss D: -32.7010, loss G: 98.8599\n",
            "Epoch [20/200] Batch 0/43 Loss D: -29.0717, loss G: 89.7697\n",
            "Epoch [20/200] Batch 20/43 Loss D: -27.4303, loss G: 97.4713\n",
            "Epoch [20/200] Batch 40/43 Loss D: -25.9943, loss G: 92.5339\n",
            "Epoch [21/200] Batch 0/43 Loss D: -29.7368, loss G: 86.5705\n",
            "Epoch [21/200] Batch 20/43 Loss D: -28.7353, loss G: 87.5397\n",
            "Epoch [21/200] Batch 40/43 Loss D: -38.1836, loss G: 96.1235\n",
            "Epoch [22/200] Batch 0/43 Loss D: -29.2161, loss G: 78.2194\n",
            "Epoch [22/200] Batch 20/43 Loss D: -23.3348, loss G: 99.9988\n",
            "Epoch [22/200] Batch 40/43 Loss D: -27.5879, loss G: 101.0365\n",
            "Epoch [23/200] Batch 0/43 Loss D: -28.0676, loss G: 93.0993\n",
            "Epoch [23/200] Batch 20/43 Loss D: -26.2177, loss G: 107.7643\n",
            "Epoch [23/200] Batch 40/43 Loss D: -24.2824, loss G: 100.8587\n",
            "Epoch [24/200] Batch 0/43 Loss D: -29.7937, loss G: 97.2816\n",
            "Epoch [24/200] Batch 20/43 Loss D: -24.9755, loss G: 102.1266\n",
            "Epoch [24/200] Batch 40/43 Loss D: -32.6394, loss G: 112.3708\n",
            "Epoch [25/200] Batch 0/43 Loss D: -32.9684, loss G: 88.6790\n",
            "Epoch [25/200] Batch 20/43 Loss D: -29.8670, loss G: 96.7578\n",
            "Epoch [25/200] Batch 40/43 Loss D: -29.0882, loss G: 98.0281\n",
            "Epoch [26/200] Batch 0/43 Loss D: -33.2228, loss G: 92.8321\n",
            "Epoch [26/200] Batch 20/43 Loss D: -34.5445, loss G: 114.5885\n",
            "Epoch [26/200] Batch 40/43 Loss D: -25.1252, loss G: 92.6958\n",
            "Epoch [27/200] Batch 0/43 Loss D: -26.6994, loss G: 102.6124\n",
            "Epoch [27/200] Batch 20/43 Loss D: -29.8928, loss G: 130.5922\n",
            "Epoch [27/200] Batch 40/43 Loss D: -28.3624, loss G: 117.9558\n",
            "Epoch [28/200] Batch 0/43 Loss D: -24.1765, loss G: 105.2904\n",
            "Epoch [28/200] Batch 20/43 Loss D: -25.5844, loss G: 111.6152\n",
            "Epoch [28/200] Batch 40/43 Loss D: -25.8931, loss G: 107.8852\n",
            "Epoch [29/200] Batch 0/43 Loss D: -32.7429, loss G: 96.8204\n",
            "Epoch [29/200] Batch 20/43 Loss D: -29.7773, loss G: 109.7047\n",
            "Epoch [29/200] Batch 40/43 Loss D: -30.2225, loss G: 116.6782\n",
            "Epoch [30/200] Batch 0/43 Loss D: -28.2821, loss G: 106.0515\n",
            "Epoch [30/200] Batch 20/43 Loss D: -26.3939, loss G: 115.8414\n",
            "Epoch [30/200] Batch 40/43 Loss D: -32.9153, loss G: 114.4839\n",
            "Epoch [31/200] Batch 0/43 Loss D: -28.9367, loss G: 101.7562\n",
            "Epoch [31/200] Batch 20/43 Loss D: -28.3741, loss G: 113.6964\n",
            "Epoch [31/200] Batch 40/43 Loss D: -37.6849, loss G: 111.4819\n",
            "Epoch [32/200] Batch 0/43 Loss D: -27.1855, loss G: 109.6594\n",
            "Epoch [32/200] Batch 20/43 Loss D: -29.2004, loss G: 113.2632\n",
            "Epoch [32/200] Batch 40/43 Loss D: -28.6563, loss G: 122.4402\n",
            "Epoch [33/200] Batch 0/43 Loss D: -27.2582, loss G: 116.8041\n",
            "Epoch [33/200] Batch 20/43 Loss D: -30.0189, loss G: 114.7873\n",
            "Epoch [33/200] Batch 40/43 Loss D: -27.4146, loss G: 114.5991\n",
            "Epoch [34/200] Batch 0/43 Loss D: -26.8804, loss G: 101.8880\n",
            "Epoch [34/200] Batch 20/43 Loss D: -28.5688, loss G: 117.2541\n",
            "Epoch [34/200] Batch 40/43 Loss D: -30.9536, loss G: 121.5292\n",
            "Epoch [35/200] Batch 0/43 Loss D: -30.3214, loss G: 100.4984\n",
            "Epoch [35/200] Batch 20/43 Loss D: -30.1368, loss G: 119.8213\n",
            "Epoch [35/200] Batch 40/43 Loss D: -30.7207, loss G: 110.4959\n",
            "Epoch [36/200] Batch 0/43 Loss D: -27.6076, loss G: 119.1457\n",
            "Epoch [36/200] Batch 20/43 Loss D: -32.6748, loss G: 105.0870\n",
            "Epoch [36/200] Batch 40/43 Loss D: -29.3175, loss G: 118.7712\n",
            "Epoch [37/200] Batch 0/43 Loss D: -32.1223, loss G: 106.2761\n",
            "Epoch [37/200] Batch 20/43 Loss D: -30.8025, loss G: 119.9163\n",
            "Epoch [37/200] Batch 40/43 Loss D: -31.2768, loss G: 126.8291\n",
            "Epoch [38/200] Batch 0/43 Loss D: -29.2787, loss G: 114.9371\n",
            "Epoch [38/200] Batch 20/43 Loss D: -30.6202, loss G: 115.8829\n",
            "Epoch [38/200] Batch 40/43 Loss D: -30.0353, loss G: 114.6768\n",
            "Epoch [39/200] Batch 0/43 Loss D: -30.0429, loss G: 103.6643\n",
            "Epoch [39/200] Batch 20/43 Loss D: -29.6933, loss G: 123.8192\n",
            "Epoch [39/200] Batch 40/43 Loss D: -29.2581, loss G: 124.2216\n",
            "Epoch [40/200] Batch 0/43 Loss D: -31.3773, loss G: 114.0983\n",
            "Epoch [40/200] Batch 20/43 Loss D: -39.2624, loss G: 124.8685\n",
            "Epoch [40/200] Batch 40/43 Loss D: -29.2575, loss G: 127.5169\n",
            "Epoch [41/200] Batch 0/43 Loss D: -32.4520, loss G: 112.2243\n",
            "Epoch [41/200] Batch 20/43 Loss D: -22.4987, loss G: 131.6314\n",
            "Epoch [41/200] Batch 40/43 Loss D: -26.4960, loss G: 128.3239\n",
            "Epoch [42/200] Batch 0/43 Loss D: -27.1577, loss G: 113.1289\n",
            "Epoch [42/200] Batch 20/43 Loss D: -27.4611, loss G: 120.2881\n",
            "Epoch [42/200] Batch 40/43 Loss D: -29.1555, loss G: 117.6021\n",
            "Epoch [43/200] Batch 0/43 Loss D: -26.8355, loss G: 119.3986\n",
            "Epoch [43/200] Batch 20/43 Loss D: -29.4577, loss G: 119.8495\n",
            "Epoch [43/200] Batch 40/43 Loss D: -30.3257, loss G: 109.5922\n",
            "Epoch [44/200] Batch 0/43 Loss D: -33.8672, loss G: 122.1207\n",
            "Epoch [44/200] Batch 20/43 Loss D: -24.5851, loss G: 114.3224\n",
            "Epoch [44/200] Batch 40/43 Loss D: -28.1409, loss G: 128.6540\n",
            "Epoch [45/200] Batch 0/43 Loss D: -34.9498, loss G: 117.2655\n",
            "Epoch [45/200] Batch 20/43 Loss D: -28.1451, loss G: 123.7181\n",
            "Epoch [45/200] Batch 40/43 Loss D: -28.2610, loss G: 125.1671\n",
            "Epoch [46/200] Batch 0/43 Loss D: -28.1516, loss G: 123.0348\n",
            "Epoch [46/200] Batch 20/43 Loss D: -35.8538, loss G: 132.2319\n",
            "Epoch [46/200] Batch 40/43 Loss D: -25.3403, loss G: 126.2913\n",
            "Epoch [47/200] Batch 0/43 Loss D: -39.3662, loss G: 114.3231\n",
            "Epoch [47/200] Batch 20/43 Loss D: -31.0359, loss G: 123.0174\n",
            "Epoch [47/200] Batch 40/43 Loss D: -31.0893, loss G: 113.2081\n",
            "Epoch [48/200] Batch 0/43 Loss D: -32.6726, loss G: 120.1000\n",
            "Epoch [48/200] Batch 20/43 Loss D: -33.2232, loss G: 125.1163\n",
            "Epoch [48/200] Batch 40/43 Loss D: -33.6449, loss G: 125.6401\n",
            "Epoch [49/200] Batch 0/43 Loss D: -31.0673, loss G: 110.9128\n",
            "Epoch [49/200] Batch 20/43 Loss D: -32.2204, loss G: 123.0837\n",
            "Epoch [49/200] Batch 40/43 Loss D: -39.4221, loss G: 123.9510\n",
            "Epoch [50/200] Batch 0/43 Loss D: -31.0846, loss G: 127.5052\n",
            "Epoch [50/200] Batch 20/43 Loss D: -27.3495, loss G: 120.0773\n",
            "Epoch [50/200] Batch 40/43 Loss D: -31.4885, loss G: 122.9201\n",
            "Epoch [51/200] Batch 0/43 Loss D: -28.3713, loss G: 126.5466\n",
            "Epoch [51/200] Batch 20/43 Loss D: -28.1396, loss G: 127.4084\n",
            "Epoch [51/200] Batch 40/43 Loss D: -25.0955, loss G: 128.4693\n",
            "Epoch [52/200] Batch 0/43 Loss D: -39.3187, loss G: 109.0911\n",
            "Epoch [52/200] Batch 20/43 Loss D: -29.6704, loss G: 122.4365\n",
            "Epoch [52/200] Batch 40/43 Loss D: -27.7957, loss G: 137.0149\n",
            "Epoch [53/200] Batch 0/43 Loss D: -27.0591, loss G: 133.9268\n",
            "Epoch [53/200] Batch 20/43 Loss D: -32.0942, loss G: 127.0198\n",
            "Epoch [53/200] Batch 40/43 Loss D: -38.2221, loss G: 117.4487\n",
            "Epoch [54/200] Batch 0/43 Loss D: -22.5497, loss G: 129.7276\n",
            "Epoch [54/200] Batch 20/43 Loss D: -30.7748, loss G: 123.1466\n",
            "Epoch [54/200] Batch 40/43 Loss D: -33.3277, loss G: 115.5592\n",
            "Epoch [55/200] Batch 0/43 Loss D: -26.5931, loss G: 128.4972\n",
            "Epoch [55/200] Batch 20/43 Loss D: -30.8127, loss G: 123.2256\n",
            "Epoch [55/200] Batch 40/43 Loss D: -30.9894, loss G: 129.7221\n",
            "Epoch [56/200] Batch 0/43 Loss D: -27.4884, loss G: 128.5840\n",
            "Epoch [56/200] Batch 20/43 Loss D: -30.7243, loss G: 126.4925\n",
            "Epoch [56/200] Batch 40/43 Loss D: -30.5658, loss G: 138.0104\n",
            "Epoch [57/200] Batch 0/43 Loss D: -30.7468, loss G: 133.3181\n",
            "Epoch [57/200] Batch 20/43 Loss D: -31.4323, loss G: 129.6345\n",
            "Epoch [57/200] Batch 40/43 Loss D: -32.5432, loss G: 139.0822\n",
            "Epoch [58/200] Batch 0/43 Loss D: -25.2063, loss G: 127.1973\n",
            "Epoch [58/200] Batch 20/43 Loss D: -32.5262, loss G: 138.3749\n",
            "Epoch [58/200] Batch 40/43 Loss D: -33.2416, loss G: 124.6700\n",
            "Epoch [59/200] Batch 0/43 Loss D: -24.5932, loss G: 136.1234\n",
            "Epoch [59/200] Batch 20/43 Loss D: -32.1592, loss G: 134.0943\n",
            "Epoch [59/200] Batch 40/43 Loss D: -27.3249, loss G: 122.3437\n",
            "Epoch [60/200] Batch 0/43 Loss D: -28.4590, loss G: 121.2935\n",
            "Epoch [60/200] Batch 20/43 Loss D: -34.0289, loss G: 131.8331\n",
            "Epoch [60/200] Batch 40/43 Loss D: -30.3396, loss G: 123.6547\n",
            "Epoch [61/200] Batch 0/43 Loss D: -30.0574, loss G: 136.8097\n",
            "Epoch [61/200] Batch 20/43 Loss D: -27.6111, loss G: 128.9029\n",
            "Epoch [61/200] Batch 40/43 Loss D: -31.4366, loss G: 119.0803\n",
            "Epoch [62/200] Batch 0/43 Loss D: -26.5679, loss G: 142.8513\n",
            "Epoch [62/200] Batch 20/43 Loss D: -31.8244, loss G: 133.2420\n",
            "Epoch [62/200] Batch 40/43 Loss D: -27.6075, loss G: 132.6653\n",
            "Epoch [63/200] Batch 0/43 Loss D: -31.8096, loss G: 134.9796\n",
            "Epoch [63/200] Batch 20/43 Loss D: -31.8418, loss G: 150.0826\n",
            "Epoch [63/200] Batch 40/43 Loss D: -29.4815, loss G: 130.5941\n",
            "Epoch [64/200] Batch 0/43 Loss D: -29.3188, loss G: 134.3064\n",
            "Epoch [64/200] Batch 20/43 Loss D: -38.0150, loss G: 133.5161\n",
            "Epoch [64/200] Batch 40/43 Loss D: -33.6137, loss G: 143.8086\n",
            "Epoch [65/200] Batch 0/43 Loss D: -33.2450, loss G: 131.6875\n",
            "Epoch [65/200] Batch 20/43 Loss D: -29.1393, loss G: 129.3357\n",
            "Epoch [65/200] Batch 40/43 Loss D: -37.3152, loss G: 131.9909\n",
            "Epoch [66/200] Batch 0/43 Loss D: -30.1647, loss G: 147.7373\n",
            "Epoch [66/200] Batch 20/43 Loss D: -38.2865, loss G: 128.3883\n",
            "Epoch [66/200] Batch 40/43 Loss D: -25.9157, loss G: 151.6516\n",
            "Epoch [67/200] Batch 0/43 Loss D: -33.8962, loss G: 144.5841\n",
            "Epoch [67/200] Batch 20/43 Loss D: -30.9109, loss G: 121.7329\n",
            "Epoch [67/200] Batch 40/43 Loss D: -34.7973, loss G: 130.3806\n",
            "Epoch [68/200] Batch 0/43 Loss D: -28.4899, loss G: 134.1024\n",
            "Epoch [68/200] Batch 20/43 Loss D: -36.9123, loss G: 125.7930\n",
            "Epoch [68/200] Batch 40/43 Loss D: -35.9325, loss G: 135.3985\n",
            "Epoch [69/200] Batch 0/43 Loss D: -36.0728, loss G: 127.1901\n",
            "Epoch [69/200] Batch 20/43 Loss D: -31.2399, loss G: 131.5487\n",
            "Epoch [69/200] Batch 40/43 Loss D: -33.1088, loss G: 157.1039\n",
            "Epoch [70/200] Batch 0/43 Loss D: -31.4436, loss G: 135.9098\n",
            "Epoch [70/200] Batch 20/43 Loss D: -32.6805, loss G: 132.8300\n",
            "Epoch [70/200] Batch 40/43 Loss D: -31.3772, loss G: 140.2023\n",
            "Epoch [71/200] Batch 0/43 Loss D: -30.2381, loss G: 136.1169\n",
            "Epoch [71/200] Batch 20/43 Loss D: -36.7003, loss G: 132.3259\n",
            "Epoch [71/200] Batch 40/43 Loss D: -28.9531, loss G: 140.4901\n",
            "Epoch [72/200] Batch 0/43 Loss D: -21.2350, loss G: 139.0347\n",
            "Epoch [72/200] Batch 20/43 Loss D: -39.6664, loss G: 129.5401\n",
            "Epoch [72/200] Batch 40/43 Loss D: -33.4095, loss G: 131.8435\n",
            "Epoch [73/200] Batch 0/43 Loss D: -33.0840, loss G: 135.7099\n",
            "Epoch [73/200] Batch 20/43 Loss D: -33.6929, loss G: 140.4570\n",
            "Epoch [73/200] Batch 40/43 Loss D: -34.3613, loss G: 126.2728\n",
            "Epoch [74/200] Batch 0/43 Loss D: -34.3272, loss G: 143.2315\n",
            "Epoch [74/200] Batch 20/43 Loss D: -28.5876, loss G: 142.3535\n",
            "Epoch [74/200] Batch 40/43 Loss D: -29.4194, loss G: 129.6161\n",
            "Epoch [75/200] Batch 0/43 Loss D: -31.7805, loss G: 135.4007\n",
            "Epoch [75/200] Batch 20/43 Loss D: -34.4908, loss G: 137.6758\n",
            "Epoch [75/200] Batch 40/43 Loss D: -35.2089, loss G: 125.2040\n",
            "Epoch [76/200] Batch 0/43 Loss D: -26.6043, loss G: 146.9642\n",
            "Epoch [76/200] Batch 20/43 Loss D: -32.0568, loss G: 149.2622\n",
            "Epoch [76/200] Batch 40/43 Loss D: -31.6955, loss G: 145.1514\n",
            "Epoch [77/200] Batch 0/43 Loss D: -28.6783, loss G: 132.0827\n",
            "Epoch [77/200] Batch 20/43 Loss D: -36.0166, loss G: 132.9877\n",
            "Epoch [77/200] Batch 40/43 Loss D: -33.3698, loss G: 141.5616\n",
            "Epoch [78/200] Batch 0/43 Loss D: -31.5574, loss G: 128.2260\n",
            "Epoch [78/200] Batch 20/43 Loss D: -29.7356, loss G: 132.0709\n",
            "Epoch [78/200] Batch 40/43 Loss D: -35.9630, loss G: 129.8659\n",
            "Epoch [79/200] Batch 0/43 Loss D: -30.2055, loss G: 127.1761\n",
            "Epoch [79/200] Batch 20/43 Loss D: -33.0427, loss G: 136.4291\n",
            "Epoch [79/200] Batch 40/43 Loss D: -35.9572, loss G: 147.3228\n",
            "Epoch [80/200] Batch 0/43 Loss D: -39.3326, loss G: 125.3081\n",
            "Epoch [80/200] Batch 20/43 Loss D: -26.8698, loss G: 139.1318\n",
            "Epoch [80/200] Batch 40/43 Loss D: -32.5604, loss G: 145.1752\n",
            "Epoch [81/200] Batch 0/43 Loss D: -36.1334, loss G: 129.7800\n",
            "Epoch [81/200] Batch 20/43 Loss D: -24.1626, loss G: 145.0327\n",
            "Epoch [81/200] Batch 40/43 Loss D: -30.5674, loss G: 137.7151\n",
            "Epoch [82/200] Batch 0/43 Loss D: -28.3751, loss G: 147.5196\n",
            "Epoch [82/200] Batch 20/43 Loss D: -35.9260, loss G: 135.7575\n",
            "Epoch [82/200] Batch 40/43 Loss D: -33.3350, loss G: 154.3780\n",
            "Epoch [83/200] Batch 0/43 Loss D: -32.3590, loss G: 130.0686\n",
            "Epoch [83/200] Batch 20/43 Loss D: -34.2560, loss G: 124.1125\n",
            "Epoch [83/200] Batch 40/43 Loss D: -31.5812, loss G: 144.0468\n",
            "Epoch [84/200] Batch 0/43 Loss D: -34.6322, loss G: 123.7976\n",
            "Epoch [84/200] Batch 20/43 Loss D: -28.3630, loss G: 142.3327\n",
            "Epoch [84/200] Batch 40/43 Loss D: -28.4669, loss G: 146.8732\n",
            "Epoch [85/200] Batch 0/43 Loss D: -26.5978, loss G: 130.7233\n",
            "Epoch [85/200] Batch 20/43 Loss D: -28.7458, loss G: 145.4092\n",
            "Epoch [85/200] Batch 40/43 Loss D: -29.9615, loss G: 136.7104\n",
            "Epoch [86/200] Batch 0/43 Loss D: -30.7988, loss G: 138.3846\n",
            "Epoch [86/200] Batch 20/43 Loss D: -32.1069, loss G: 141.4388\n",
            "Epoch [86/200] Batch 40/43 Loss D: -29.5051, loss G: 140.6088\n",
            "Epoch [87/200] Batch 0/43 Loss D: -34.9220, loss G: 143.9912\n",
            "Epoch [87/200] Batch 20/43 Loss D: -27.8153, loss G: 144.5309\n",
            "Epoch [87/200] Batch 40/43 Loss D: -34.7650, loss G: 128.4885\n",
            "Epoch [88/200] Batch 0/43 Loss D: -27.6287, loss G: 138.9413\n",
            "Epoch [88/200] Batch 20/43 Loss D: -25.4568, loss G: 140.8211\n",
            "Epoch [88/200] Batch 40/43 Loss D: -29.3235, loss G: 149.3325\n",
            "Epoch [89/200] Batch 0/43 Loss D: -32.9736, loss G: 136.8588\n",
            "Epoch [89/200] Batch 20/43 Loss D: -28.4403, loss G: 135.0784\n",
            "Epoch [89/200] Batch 40/43 Loss D: -35.4446, loss G: 151.4236\n",
            "Epoch [90/200] Batch 0/43 Loss D: -27.9318, loss G: 143.5069\n",
            "Epoch [90/200] Batch 20/43 Loss D: -33.4386, loss G: 133.1351\n",
            "Epoch [90/200] Batch 40/43 Loss D: -35.0578, loss G: 127.1234\n",
            "Epoch [91/200] Batch 0/43 Loss D: -29.5724, loss G: 138.7636\n",
            "Epoch [91/200] Batch 20/43 Loss D: -29.8670, loss G: 142.5813\n",
            "Epoch [91/200] Batch 40/43 Loss D: -36.9687, loss G: 143.6254\n",
            "Epoch [92/200] Batch 0/43 Loss D: -31.2893, loss G: 145.1206\n",
            "Epoch [92/200] Batch 20/43 Loss D: -32.7639, loss G: 145.6866\n",
            "Epoch [92/200] Batch 40/43 Loss D: -34.0504, loss G: 149.2158\n",
            "Epoch [93/200] Batch 0/43 Loss D: -27.8462, loss G: 138.0822\n",
            "Epoch [93/200] Batch 20/43 Loss D: -32.3037, loss G: 139.8506\n",
            "Epoch [93/200] Batch 40/43 Loss D: -27.3123, loss G: 146.2068\n",
            "Epoch [94/200] Batch 0/43 Loss D: -30.1209, loss G: 136.5526\n",
            "Epoch [94/200] Batch 20/43 Loss D: -29.9967, loss G: 141.5006\n",
            "Epoch [94/200] Batch 40/43 Loss D: -36.9441, loss G: 145.5169\n",
            "Epoch [95/200] Batch 0/43 Loss D: -30.0051, loss G: 150.7712\n",
            "Epoch [95/200] Batch 20/43 Loss D: -30.8431, loss G: 140.8755\n",
            "Epoch [95/200] Batch 40/43 Loss D: -35.4071, loss G: 149.9953\n",
            "Epoch [96/200] Batch 0/43 Loss D: -30.2755, loss G: 146.0882\n",
            "Epoch [96/200] Batch 20/43 Loss D: -30.9099, loss G: 141.6189\n",
            "Epoch [96/200] Batch 40/43 Loss D: -39.9480, loss G: 140.9725\n",
            "Epoch [97/200] Batch 0/43 Loss D: -25.7987, loss G: 155.7946\n",
            "Epoch [97/200] Batch 20/43 Loss D: -32.3465, loss G: 151.6911\n",
            "Epoch [97/200] Batch 40/43 Loss D: -25.5489, loss G: 152.9563\n",
            "Epoch [98/200] Batch 0/43 Loss D: -36.0816, loss G: 135.8671\n",
            "Epoch [98/200] Batch 20/43 Loss D: -30.6109, loss G: 133.4037\n",
            "Epoch [98/200] Batch 40/43 Loss D: -28.7445, loss G: 139.6249\n",
            "Epoch [99/200] Batch 0/43 Loss D: -29.5814, loss G: 138.1863\n",
            "Epoch [99/200] Batch 20/43 Loss D: -42.6695, loss G: 137.2421\n",
            "Epoch [99/200] Batch 40/43 Loss D: -35.3953, loss G: 139.2633\n",
            "Epoch [100/200] Batch 0/43 Loss D: -27.9791, loss G: 152.0575\n",
            "Epoch [100/200] Batch 20/43 Loss D: -30.1036, loss G: 148.9864\n",
            "Epoch [100/200] Batch 40/43 Loss D: -32.5993, loss G: 153.9129\n",
            "Epoch [101/200] Batch 0/43 Loss D: -35.6644, loss G: 144.6659\n",
            "Epoch [101/200] Batch 20/43 Loss D: -32.0430, loss G: 142.8639\n",
            "Epoch [101/200] Batch 40/43 Loss D: -29.6544, loss G: 143.7288\n",
            "Epoch [102/200] Batch 0/43 Loss D: -27.9545, loss G: 141.1023\n",
            "Epoch [102/200] Batch 20/43 Loss D: -37.7271, loss G: 133.7868\n",
            "Epoch [102/200] Batch 40/43 Loss D: -25.5237, loss G: 147.1196\n",
            "Epoch [103/200] Batch 0/43 Loss D: -32.6719, loss G: 139.3091\n",
            "Epoch [103/200] Batch 20/43 Loss D: -41.9904, loss G: 131.7718\n",
            "Epoch [103/200] Batch 40/43 Loss D: -34.2528, loss G: 149.0838\n",
            "Epoch [104/200] Batch 0/43 Loss D: -35.5244, loss G: 137.1945\n",
            "Epoch [104/200] Batch 20/43 Loss D: -31.3412, loss G: 155.4323\n",
            "Epoch [104/200] Batch 40/43 Loss D: -32.2469, loss G: 141.8193\n",
            "Epoch [105/200] Batch 0/43 Loss D: -36.5821, loss G: 140.8734\n",
            "Epoch [105/200] Batch 20/43 Loss D: -38.3396, loss G: 148.4517\n",
            "Epoch [105/200] Batch 40/43 Loss D: -31.1702, loss G: 148.8156\n",
            "Epoch [106/200] Batch 0/43 Loss D: -28.2215, loss G: 146.4742\n",
            "Epoch [106/200] Batch 20/43 Loss D: -32.7067, loss G: 147.0251\n",
            "Epoch [106/200] Batch 40/43 Loss D: -35.4793, loss G: 146.2936\n",
            "Epoch [107/200] Batch 0/43 Loss D: -30.5114, loss G: 146.8493\n",
            "Epoch [107/200] Batch 20/43 Loss D: -28.0103, loss G: 141.0085\n",
            "Epoch [107/200] Batch 40/43 Loss D: -26.3627, loss G: 154.0423\n",
            "Epoch [108/200] Batch 0/43 Loss D: -31.9674, loss G: 140.0615\n",
            "Epoch [108/200] Batch 20/43 Loss D: -37.2859, loss G: 154.3176\n",
            "Epoch [108/200] Batch 40/43 Loss D: -26.8718, loss G: 149.8884\n",
            "Epoch [109/200] Batch 0/43 Loss D: -35.2363, loss G: 139.2991\n",
            "Epoch [109/200] Batch 20/43 Loss D: -32.8613, loss G: 145.0963\n",
            "Epoch [109/200] Batch 40/43 Loss D: -37.6970, loss G: 148.6534\n",
            "Epoch [110/200] Batch 0/43 Loss D: -29.5573, loss G: 160.8971\n",
            "Epoch [110/200] Batch 20/43 Loss D: -43.0213, loss G: 135.7778\n",
            "Epoch [110/200] Batch 40/43 Loss D: -23.6663, loss G: 150.8287\n",
            "Epoch [111/200] Batch 0/43 Loss D: -33.0075, loss G: 139.3539\n",
            "Epoch [111/200] Batch 20/43 Loss D: -30.1300, loss G: 151.4373\n",
            "Epoch [111/200] Batch 40/43 Loss D: -25.4947, loss G: 148.0692\n",
            "Epoch [112/200] Batch 0/43 Loss D: -26.8362, loss G: 157.0185\n",
            "Epoch [112/200] Batch 20/43 Loss D: -34.1327, loss G: 145.6346\n",
            "Epoch [112/200] Batch 40/43 Loss D: -33.8899, loss G: 148.3134\n",
            "Epoch [113/200] Batch 0/43 Loss D: -32.2922, loss G: 150.2713\n",
            "Epoch [113/200] Batch 20/43 Loss D: -37.3231, loss G: 159.3766\n",
            "Epoch [113/200] Batch 40/43 Loss D: -26.2245, loss G: 146.1315\n",
            "Epoch [114/200] Batch 0/43 Loss D: -31.9588, loss G: 143.6785\n",
            "Epoch [114/200] Batch 20/43 Loss D: -33.7514, loss G: 147.8777\n",
            "Epoch [114/200] Batch 40/43 Loss D: -32.8667, loss G: 150.4694\n",
            "Epoch [115/200] Batch 0/43 Loss D: -27.6025, loss G: 164.6420\n",
            "Epoch [115/200] Batch 20/43 Loss D: -38.9922, loss G: 144.4068\n",
            "Epoch [115/200] Batch 40/43 Loss D: -25.8855, loss G: 147.6127\n",
            "Epoch [116/200] Batch 0/43 Loss D: -31.3736, loss G: 157.8936\n",
            "Epoch [116/200] Batch 20/43 Loss D: -33.2421, loss G: 163.8855\n",
            "Epoch [116/200] Batch 40/43 Loss D: -40.0120, loss G: 136.7198\n",
            "Epoch [117/200] Batch 0/43 Loss D: -32.5452, loss G: 142.0684\n",
            "Epoch [117/200] Batch 20/43 Loss D: -27.1169, loss G: 155.6967\n",
            "Epoch [117/200] Batch 40/43 Loss D: -25.0227, loss G: 153.3054\n",
            "Epoch [118/200] Batch 0/43 Loss D: -29.0902, loss G: 148.8722\n",
            "Epoch [118/200] Batch 20/43 Loss D: -33.3464, loss G: 137.3510\n",
            "Epoch [118/200] Batch 40/43 Loss D: -32.6712, loss G: 149.4892\n",
            "Epoch [119/200] Batch 0/43 Loss D: -25.1073, loss G: 152.1090\n",
            "Epoch [119/200] Batch 20/43 Loss D: -36.2932, loss G: 147.8327\n",
            "Epoch [119/200] Batch 40/43 Loss D: -37.6467, loss G: 148.9931\n",
            "Epoch [120/200] Batch 0/43 Loss D: -35.4021, loss G: 156.9768\n",
            "Epoch [120/200] Batch 20/43 Loss D: -35.0091, loss G: 149.8745\n",
            "Epoch [120/200] Batch 40/43 Loss D: -32.6988, loss G: 147.1441\n",
            "Epoch [121/200] Batch 0/43 Loss D: -32.2510, loss G: 152.5643\n",
            "Epoch [121/200] Batch 20/43 Loss D: -31.6256, loss G: 145.3020\n",
            "Epoch [121/200] Batch 40/43 Loss D: -30.0681, loss G: 153.3248\n",
            "Epoch [122/200] Batch 0/43 Loss D: -30.8770, loss G: 153.2729\n",
            "Epoch [122/200] Batch 20/43 Loss D: -28.6408, loss G: 161.2844\n",
            "Epoch [122/200] Batch 40/43 Loss D: -32.7444, loss G: 156.3866\n",
            "Epoch [123/200] Batch 0/43 Loss D: -40.8877, loss G: 148.5367\n",
            "Epoch [123/200] Batch 20/43 Loss D: -31.0684, loss G: 148.7652\n",
            "Epoch [123/200] Batch 40/43 Loss D: -33.0622, loss G: 144.6311\n",
            "Epoch [124/200] Batch 0/43 Loss D: -36.5414, loss G: 171.1523\n",
            "Epoch [124/200] Batch 20/43 Loss D: -29.0022, loss G: 155.2818\n",
            "Epoch [124/200] Batch 40/43 Loss D: -30.1574, loss G: 154.2440\n",
            "Epoch [125/200] Batch 0/43 Loss D: -35.7625, loss G: 143.7096\n",
            "Epoch [125/200] Batch 20/43 Loss D: -36.8629, loss G: 149.0119\n",
            "Epoch [125/200] Batch 40/43 Loss D: -29.3374, loss G: 147.9694\n",
            "Epoch [126/200] Batch 0/43 Loss D: -24.8965, loss G: 149.5958\n",
            "Epoch [126/200] Batch 20/43 Loss D: -38.4429, loss G: 155.0914\n",
            "Epoch [126/200] Batch 40/43 Loss D: -32.5205, loss G: 156.0911\n",
            "Epoch [127/200] Batch 0/43 Loss D: -32.7077, loss G: 156.6661\n",
            "Epoch [127/200] Batch 20/43 Loss D: -31.7396, loss G: 159.4619\n",
            "Epoch [127/200] Batch 40/43 Loss D: -32.1674, loss G: 155.7006\n",
            "Epoch [128/200] Batch 0/43 Loss D: -36.6915, loss G: 163.6312\n",
            "Epoch [128/200] Batch 20/43 Loss D: -32.7117, loss G: 162.1956\n",
            "Epoch [128/200] Batch 40/43 Loss D: -32.1022, loss G: 151.4994\n",
            "Epoch [129/200] Batch 0/43 Loss D: -31.4307, loss G: 165.0926\n",
            "Epoch [129/200] Batch 20/43 Loss D: -27.5897, loss G: 159.1767\n",
            "Epoch [129/200] Batch 40/43 Loss D: -39.6753, loss G: 157.4164\n",
            "Epoch [130/200] Batch 0/43 Loss D: -35.9879, loss G: 135.9760\n",
            "Epoch [130/200] Batch 20/43 Loss D: -39.5956, loss G: 156.3081\n",
            "Epoch [130/200] Batch 40/43 Loss D: -30.4969, loss G: 151.0375\n",
            "Epoch [131/200] Batch 0/43 Loss D: -27.8397, loss G: 147.8610\n",
            "Epoch [131/200] Batch 20/43 Loss D: -37.9554, loss G: 146.5211\n",
            "Epoch [131/200] Batch 40/43 Loss D: -31.1163, loss G: 160.0866\n",
            "Epoch [132/200] Batch 0/43 Loss D: -27.9602, loss G: 155.0162\n",
            "Epoch [132/200] Batch 20/43 Loss D: -32.3057, loss G: 158.4766\n",
            "Epoch [132/200] Batch 40/43 Loss D: -35.7417, loss G: 152.9914\n",
            "Epoch [133/200] Batch 0/43 Loss D: -30.4595, loss G: 149.7851\n",
            "Epoch [133/200] Batch 20/43 Loss D: -33.8170, loss G: 148.8874\n",
            "Epoch [133/200] Batch 40/43 Loss D: -31.8684, loss G: 148.7298\n",
            "Epoch [134/200] Batch 0/43 Loss D: -26.1474, loss G: 167.6855\n",
            "Epoch [134/200] Batch 20/43 Loss D: -39.9540, loss G: 155.5576\n",
            "Epoch [134/200] Batch 40/43 Loss D: -31.9520, loss G: 146.6326\n",
            "Epoch [135/200] Batch 0/43 Loss D: -30.0926, loss G: 179.7561\n",
            "Epoch [135/200] Batch 20/43 Loss D: -40.2223, loss G: 145.6279\n",
            "Epoch [135/200] Batch 40/43 Loss D: -32.0087, loss G: 146.7661\n",
            "Epoch [136/200] Batch 0/43 Loss D: -38.8593, loss G: 148.7565\n",
            "Epoch [136/200] Batch 20/43 Loss D: -32.6691, loss G: 158.1035\n",
            "Epoch [136/200] Batch 40/43 Loss D: -27.9020, loss G: 154.5359\n",
            "Epoch [137/200] Batch 0/43 Loss D: -31.2019, loss G: 155.6017\n",
            "Epoch [137/200] Batch 20/43 Loss D: -28.2253, loss G: 155.6601\n",
            "Epoch [137/200] Batch 40/43 Loss D: -44.7886, loss G: 148.4795\n",
            "Epoch [138/200] Batch 0/43 Loss D: -37.3377, loss G: 151.7531\n",
            "Epoch [138/200] Batch 20/43 Loss D: -34.0603, loss G: 163.3787\n",
            "Epoch [138/200] Batch 40/43 Loss D: -31.8968, loss G: 157.8635\n",
            "Epoch [139/200] Batch 0/43 Loss D: -28.2831, loss G: 149.7335\n",
            "Epoch [139/200] Batch 20/43 Loss D: -30.4533, loss G: 148.3149\n",
            "Epoch [139/200] Batch 40/43 Loss D: -28.9790, loss G: 156.5730\n",
            "Epoch [140/200] Batch 0/43 Loss D: -29.0946, loss G: 159.4813\n",
            "Epoch [140/200] Batch 20/43 Loss D: -30.5416, loss G: 152.6443\n",
            "Epoch [140/200] Batch 40/43 Loss D: -35.7893, loss G: 150.3331\n",
            "Epoch [141/200] Batch 0/43 Loss D: -29.2341, loss G: 154.7572\n",
            "Epoch [141/200] Batch 20/43 Loss D: -32.6371, loss G: 160.8714\n",
            "Epoch [141/200] Batch 40/43 Loss D: -33.9285, loss G: 153.0080\n",
            "Epoch [142/200] Batch 0/43 Loss D: -29.7766, loss G: 155.3987\n",
            "Epoch [142/200] Batch 20/43 Loss D: -35.4654, loss G: 148.0130\n",
            "Epoch [142/200] Batch 40/43 Loss D: -23.6819, loss G: 155.7077\n",
            "Epoch [143/200] Batch 0/43 Loss D: -32.4412, loss G: 162.4116\n",
            "Epoch [143/200] Batch 20/43 Loss D: -33.4450, loss G: 156.0603\n",
            "Epoch [143/200] Batch 40/43 Loss D: -32.4064, loss G: 155.0492\n",
            "Epoch [144/200] Batch 0/43 Loss D: -27.8709, loss G: 149.7830\n",
            "Epoch [144/200] Batch 20/43 Loss D: -42.9541, loss G: 154.1246\n",
            "Epoch [144/200] Batch 40/43 Loss D: -35.9095, loss G: 152.7997\n",
            "Epoch [145/200] Batch 0/43 Loss D: -26.7644, loss G: 152.7792\n",
            "Epoch [145/200] Batch 20/43 Loss D: -31.2918, loss G: 161.5060\n",
            "Epoch [145/200] Batch 40/43 Loss D: -28.4821, loss G: 162.9132\n",
            "Epoch [146/200] Batch 0/43 Loss D: -30.9329, loss G: 167.2420\n",
            "Epoch [146/200] Batch 20/43 Loss D: -37.5315, loss G: 151.8084\n",
            "Epoch [146/200] Batch 40/43 Loss D: -39.7889, loss G: 165.3596\n",
            "Epoch [147/200] Batch 0/43 Loss D: -31.4703, loss G: 145.9618\n",
            "Epoch [147/200] Batch 20/43 Loss D: -25.7410, loss G: 161.7643\n",
            "Epoch [147/200] Batch 40/43 Loss D: -28.9791, loss G: 157.6144\n",
            "Epoch [148/200] Batch 0/43 Loss D: -43.6131, loss G: 140.2966\n",
            "Epoch [148/200] Batch 20/43 Loss D: -31.7156, loss G: 166.6148\n",
            "Epoch [148/200] Batch 40/43 Loss D: -27.4768, loss G: 170.5977\n",
            "Epoch [149/200] Batch 0/43 Loss D: -26.3034, loss G: 162.5432\n",
            "Epoch [149/200] Batch 20/43 Loss D: -39.5636, loss G: 167.4904\n",
            "Epoch [149/200] Batch 40/43 Loss D: -29.6516, loss G: 146.7786\n",
            "Epoch [150/200] Batch 0/43 Loss D: -32.6558, loss G: 158.8185\n",
            "Epoch [150/200] Batch 20/43 Loss D: -35.1074, loss G: 161.7809\n",
            "Epoch [150/200] Batch 40/43 Loss D: -24.9769, loss G: 150.3053\n",
            "Epoch [151/200] Batch 0/43 Loss D: -26.0341, loss G: 161.5545\n",
            "Epoch [151/200] Batch 20/43 Loss D: -33.7171, loss G: 147.5697\n",
            "Epoch [151/200] Batch 40/43 Loss D: -33.7303, loss G: 156.7134\n",
            "Epoch [152/200] Batch 0/43 Loss D: -33.5222, loss G: 161.6884\n",
            "Epoch [152/200] Batch 20/43 Loss D: -24.8399, loss G: 158.2161\n",
            "Epoch [152/200] Batch 40/43 Loss D: -36.3508, loss G: 164.4828\n",
            "Epoch [153/200] Batch 0/43 Loss D: -30.0859, loss G: 144.2318\n",
            "Epoch [153/200] Batch 20/43 Loss D: -35.6648, loss G: 177.0609\n",
            "Epoch [153/200] Batch 40/43 Loss D: -35.3914, loss G: 163.5136\n",
            "Epoch [154/200] Batch 0/43 Loss D: -26.8861, loss G: 156.2150\n",
            "Epoch [154/200] Batch 20/43 Loss D: -39.1270, loss G: 160.9424\n",
            "Epoch [154/200] Batch 40/43 Loss D: -31.2428, loss G: 152.8337\n",
            "Epoch [155/200] Batch 0/43 Loss D: -32.8167, loss G: 162.4510\n",
            "Epoch [155/200] Batch 20/43 Loss D: -32.2876, loss G: 154.4417\n",
            "Epoch [155/200] Batch 40/43 Loss D: -35.7393, loss G: 154.6855\n",
            "Epoch [156/200] Batch 0/43 Loss D: -33.2414, loss G: 150.8900\n",
            "Epoch [156/200] Batch 20/43 Loss D: -39.6291, loss G: 154.6657\n",
            "Epoch [156/200] Batch 40/43 Loss D: -30.6755, loss G: 154.5115\n",
            "Epoch [157/200] Batch 0/43 Loss D: -37.1117, loss G: 144.9815\n",
            "Epoch [157/200] Batch 20/43 Loss D: -41.8917, loss G: 157.3309\n",
            "Epoch [157/200] Batch 40/43 Loss D: -26.6150, loss G: 159.4234\n",
            "Epoch [158/200] Batch 0/43 Loss D: -35.4720, loss G: 158.3787\n",
            "Epoch [158/200] Batch 20/43 Loss D: -36.8548, loss G: 167.8824\n",
            "Epoch [158/200] Batch 40/43 Loss D: -35.3433, loss G: 159.8679\n",
            "Epoch [159/200] Batch 0/43 Loss D: -31.8557, loss G: 165.7115\n",
            "Epoch [159/200] Batch 20/43 Loss D: -32.9647, loss G: 165.0636\n",
            "Epoch [159/200] Batch 40/43 Loss D: -33.6442, loss G: 156.9324\n",
            "Epoch [160/200] Batch 0/43 Loss D: -27.5463, loss G: 154.7356\n",
            "Epoch [160/200] Batch 20/43 Loss D: -38.3130, loss G: 165.1895\n",
            "Epoch [160/200] Batch 40/43 Loss D: -27.0842, loss G: 176.1334\n",
            "Epoch [161/200] Batch 0/43 Loss D: -32.7760, loss G: 158.8805\n",
            "Epoch [161/200] Batch 20/43 Loss D: -28.6335, loss G: 167.7037\n",
            "Epoch [161/200] Batch 40/43 Loss D: -44.2453, loss G: 153.3690\n",
            "Epoch [162/200] Batch 0/43 Loss D: -28.4817, loss G: 172.6193\n",
            "Epoch [162/200] Batch 20/43 Loss D: -36.4238, loss G: 143.4036\n",
            "Epoch [162/200] Batch 40/43 Loss D: -30.6155, loss G: 174.3255\n",
            "Epoch [163/200] Batch 0/43 Loss D: -41.6303, loss G: 160.1973\n",
            "Epoch [163/200] Batch 20/43 Loss D: -39.9013, loss G: 156.9895\n",
            "Epoch [163/200] Batch 40/43 Loss D: -33.1082, loss G: 160.6252\n",
            "Epoch [164/200] Batch 0/43 Loss D: -37.3006, loss G: 150.5485\n",
            "Epoch [164/200] Batch 20/43 Loss D: -37.9175, loss G: 149.2416\n",
            "Epoch [164/200] Batch 40/43 Loss D: -36.8208, loss G: 144.4238\n",
            "Epoch [165/200] Batch 0/43 Loss D: -36.6261, loss G: 160.6736\n",
            "Epoch [165/200] Batch 20/43 Loss D: -36.3575, loss G: 162.1174\n",
            "Epoch [165/200] Batch 40/43 Loss D: -35.4079, loss G: 146.0279\n",
            "Epoch [166/200] Batch 0/43 Loss D: -30.8019, loss G: 161.5251\n",
            "Epoch [166/200] Batch 20/43 Loss D: -32.1554, loss G: 168.6786\n",
            "Epoch [166/200] Batch 40/43 Loss D: -31.1702, loss G: 158.4908\n",
            "Epoch [167/200] Batch 0/43 Loss D: -35.3082, loss G: 165.5151\n",
            "Epoch [167/200] Batch 20/43 Loss D: -31.8535, loss G: 166.1922\n",
            "Epoch [167/200] Batch 40/43 Loss D: -35.4986, loss G: 164.6738\n",
            "Epoch [168/200] Batch 0/43 Loss D: -32.5698, loss G: 157.9171\n",
            "Epoch [168/200] Batch 20/43 Loss D: -31.5083, loss G: 151.2186\n",
            "Epoch [168/200] Batch 40/43 Loss D: -33.9381, loss G: 158.2513\n",
            "Epoch [169/200] Batch 0/43 Loss D: -32.9342, loss G: 159.5960\n",
            "Epoch [169/200] Batch 20/43 Loss D: -26.2179, loss G: 177.7049\n",
            "Epoch [169/200] Batch 40/43 Loss D: -38.2262, loss G: 158.1057\n",
            "Epoch [170/200] Batch 0/43 Loss D: -29.7362, loss G: 177.6180\n",
            "Epoch [170/200] Batch 20/43 Loss D: -34.3495, loss G: 168.4604\n",
            "Epoch [170/200] Batch 40/43 Loss D: -27.9563, loss G: 164.3323\n",
            "Epoch [171/200] Batch 0/43 Loss D: -35.0970, loss G: 157.8029\n",
            "Epoch [171/200] Batch 20/43 Loss D: -32.1614, loss G: 160.7279\n",
            "Epoch [171/200] Batch 40/43 Loss D: -38.1309, loss G: 146.3568\n",
            "Epoch [172/200] Batch 0/43 Loss D: -30.4257, loss G: 148.3016\n",
            "Epoch [172/200] Batch 20/43 Loss D: -31.3629, loss G: 164.5762\n",
            "Epoch [172/200] Batch 40/43 Loss D: -33.2070, loss G: 166.8337\n",
            "Epoch [173/200] Batch 0/43 Loss D: -36.2602, loss G: 159.0263\n",
            "Epoch [173/200] Batch 20/43 Loss D: -35.4319, loss G: 152.1649\n",
            "Epoch [173/200] Batch 40/43 Loss D: -25.3386, loss G: 162.7398\n",
            "Epoch [174/200] Batch 0/43 Loss D: -32.5990, loss G: 164.0437\n",
            "Epoch [174/200] Batch 20/43 Loss D: -37.3016, loss G: 158.9505\n",
            "Epoch [174/200] Batch 40/43 Loss D: -31.2103, loss G: 171.9657\n",
            "Epoch [175/200] Batch 0/43 Loss D: -35.8342, loss G: 159.7963\n",
            "Epoch [175/200] Batch 20/43 Loss D: -30.8514, loss G: 159.5457\n",
            "Epoch [175/200] Batch 40/43 Loss D: -23.3510, loss G: 161.3637\n",
            "Epoch [176/200] Batch 0/43 Loss D: -33.3785, loss G: 158.8792\n",
            "Epoch [176/200] Batch 20/43 Loss D: -30.2505, loss G: 168.9951\n",
            "Epoch [176/200] Batch 40/43 Loss D: -33.8083, loss G: 165.6173\n",
            "Epoch [177/200] Batch 0/43 Loss D: -29.3112, loss G: 155.1445\n",
            "Epoch [177/200] Batch 20/43 Loss D: -29.3468, loss G: 163.5662\n",
            "Epoch [177/200] Batch 40/43 Loss D: -35.2682, loss G: 160.7015\n",
            "Epoch [178/200] Batch 0/43 Loss D: -32.6511, loss G: 157.1224\n",
            "Epoch [178/200] Batch 20/43 Loss D: -33.2451, loss G: 154.9516\n",
            "Epoch [178/200] Batch 40/43 Loss D: -32.9335, loss G: 150.7154\n",
            "Epoch [179/200] Batch 0/43 Loss D: -32.1336, loss G: 170.1490\n",
            "Epoch [179/200] Batch 20/43 Loss D: -29.4481, loss G: 160.8729\n",
            "Epoch [179/200] Batch 40/43 Loss D: -33.1546, loss G: 163.4349\n",
            "Epoch [180/200] Batch 0/43 Loss D: -34.4035, loss G: 152.5462\n",
            "Epoch [180/200] Batch 20/43 Loss D: -39.2955, loss G: 167.2886\n",
            "Epoch [180/200] Batch 40/43 Loss D: -35.6213, loss G: 158.8564\n",
            "Epoch [181/200] Batch 0/43 Loss D: -27.5169, loss G: 162.0307\n",
            "Epoch [181/200] Batch 20/43 Loss D: -29.5963, loss G: 158.2996\n",
            "Epoch [181/200] Batch 40/43 Loss D: -39.3343, loss G: 156.6061\n",
            "Epoch [182/200] Batch 0/43 Loss D: -34.4945, loss G: 165.1253\n",
            "Epoch [182/200] Batch 20/43 Loss D: -32.7494, loss G: 164.6593\n",
            "Epoch [182/200] Batch 40/43 Loss D: -27.5619, loss G: 176.1584\n",
            "Epoch [183/200] Batch 0/43 Loss D: -34.9165, loss G: 154.2492\n",
            "Epoch [183/200] Batch 20/43 Loss D: -33.5484, loss G: 153.4813\n",
            "Epoch [183/200] Batch 40/43 Loss D: -42.7491, loss G: 168.1731\n",
            "Epoch [184/200] Batch 0/43 Loss D: -34.9061, loss G: 156.6634\n",
            "Epoch [184/200] Batch 20/43 Loss D: -33.2315, loss G: 164.3574\n",
            "Epoch [184/200] Batch 40/43 Loss D: -27.7894, loss G: 165.0293\n",
            "Epoch [185/200] Batch 0/43 Loss D: -34.8498, loss G: 162.4687\n",
            "Epoch [185/200] Batch 20/43 Loss D: -42.2717, loss G: 165.8101\n",
            "Epoch [185/200] Batch 40/43 Loss D: -31.2662, loss G: 170.7702\n",
            "Epoch [186/200] Batch 0/43 Loss D: -32.6934, loss G: 159.1230\n",
            "Epoch [186/200] Batch 20/43 Loss D: -32.3055, loss G: 155.2321\n",
            "Epoch [186/200] Batch 40/43 Loss D: -30.2023, loss G: 155.4235\n",
            "Epoch [187/200] Batch 0/43 Loss D: -29.1189, loss G: 155.5840\n",
            "Epoch [187/200] Batch 20/43 Loss D: -39.5516, loss G: 161.2877\n",
            "Epoch [187/200] Batch 40/43 Loss D: -30.0463, loss G: 165.8262\n",
            "Epoch [188/200] Batch 0/43 Loss D: -38.6530, loss G: 156.5678\n",
            "Epoch [188/200] Batch 20/43 Loss D: -37.6280, loss G: 152.7654\n",
            "Epoch [188/200] Batch 40/43 Loss D: -34.1362, loss G: 165.1061\n",
            "Epoch [189/200] Batch 0/43 Loss D: -39.5156, loss G: 157.3381\n",
            "Epoch [189/200] Batch 20/43 Loss D: -34.3031, loss G: 158.8871\n",
            "Epoch [189/200] Batch 40/43 Loss D: -26.9923, loss G: 161.0342\n",
            "Epoch [190/200] Batch 0/43 Loss D: -28.8924, loss G: 160.8363\n",
            "Epoch [190/200] Batch 20/43 Loss D: -35.2157, loss G: 164.9318\n",
            "Epoch [190/200] Batch 40/43 Loss D: -30.4097, loss G: 164.5853\n",
            "Epoch [191/200] Batch 0/43 Loss D: -26.7886, loss G: 163.4247\n",
            "Epoch [191/200] Batch 20/43 Loss D: -26.3845, loss G: 169.5901\n",
            "Epoch [191/200] Batch 40/43 Loss D: -24.8439, loss G: 178.0320\n",
            "Epoch [192/200] Batch 0/43 Loss D: -34.9312, loss G: 170.1489\n",
            "Epoch [192/200] Batch 20/43 Loss D: -36.7665, loss G: 155.8994\n",
            "Epoch [192/200] Batch 40/43 Loss D: -32.6120, loss G: 154.8259\n",
            "Epoch [193/200] Batch 0/43 Loss D: -32.3377, loss G: 155.4485\n",
            "Epoch [193/200] Batch 20/43 Loss D: -30.2575, loss G: 159.8285\n",
            "Epoch [193/200] Batch 40/43 Loss D: -41.3489, loss G: 160.1459\n",
            "Epoch [194/200] Batch 0/43 Loss D: -31.5426, loss G: 166.4122\n",
            "Epoch [194/200] Batch 20/43 Loss D: -40.6829, loss G: 163.6386\n",
            "Epoch [194/200] Batch 40/43 Loss D: -30.0949, loss G: 167.6143\n",
            "Epoch [195/200] Batch 0/43 Loss D: -35.4143, loss G: 165.1182\n",
            "Epoch [195/200] Batch 20/43 Loss D: -32.2521, loss G: 171.2408\n",
            "Epoch [195/200] Batch 40/43 Loss D: -32.8920, loss G: 159.4754\n",
            "Epoch [196/200] Batch 0/43 Loss D: -27.8723, loss G: 162.1850\n",
            "Epoch [196/200] Batch 20/43 Loss D: -29.6716, loss G: 164.3415\n",
            "Epoch [196/200] Batch 40/43 Loss D: -37.3356, loss G: 150.3252\n",
            "Epoch [197/200] Batch 0/43 Loss D: -30.3524, loss G: 155.4728\n",
            "Epoch [197/200] Batch 20/43 Loss D: -27.8274, loss G: 164.3878\n",
            "Epoch [197/200] Batch 40/43 Loss D: -34.1165, loss G: 164.4413\n",
            "Epoch [198/200] Batch 0/43 Loss D: -29.5518, loss G: 162.5252\n",
            "Epoch [198/200] Batch 20/43 Loss D: -28.6329, loss G: 165.1118\n",
            "Epoch [198/200] Batch 40/43 Loss D: -35.0210, loss G: 159.3648\n",
            "Epoch [199/200] Batch 0/43 Loss D: -32.2293, loss G: 175.1248\n",
            "Epoch [199/200] Batch 20/43 Loss D: -34.0470, loss G: 161.2134\n",
            "Epoch [199/200] Batch 40/43 Loss D: -29.9732, loss G: 164.8362\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}