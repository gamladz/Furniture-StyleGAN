{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conditional_WGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdM5qDI3kpYH",
        "outputId": "8a3de7b2-6737-4c5d-d6e2-de2fb7b52f6a"
      },
      "source": [
        "!pip install tensorboard"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.36.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.32.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.19.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.12.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (54.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard) (3.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.2.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (3.4.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5u24D6jk1xL"
      },
      "source": [
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "import torch\n",
        "import glob\n",
        "import os\n",
        "import platform\n",
        "from __future__ import print_function\n",
        "import zipfile\n",
        "from zipfile import ZipFile\n",
        "import json\n",
        "\n",
        "import argparse\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "from Architectures.Conditional_WGAN_net import Generator, Discriminator, weights_init\n",
        "from Architectures.Conditional_WGAN_net import gradient_penalty\n",
        "from ImageDataset import ImageDataset\n",
        "\n",
        "%matplotlib inline\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnWdV2ohsOQ1",
        "outputId": "4a3f21f3-5673-44b3-b1ce-66b7890aa623"
      },
      "source": [
        "dataset = ImageDataset('DATA')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chair_ikea\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp-pTttBsRJ7"
      },
      "source": [
        "# Decide which device we want to run on\n",
        "ngpu = 1\n",
        "workers = 2\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "learning_rate = 1e-4\n",
        "beta1 = 0.0\n",
        "batch_size = 64\n",
        "image_size = 64\n",
        "img_ch = 3\n",
        "num_classes = dataset.num_classes\n",
        "gen_embedding = 100\n",
        "z_dim = 100\n",
        "num_epochs = 1000\n",
        "features_g = 64\n",
        "features_d = 64\n",
        "critic_iterations = 5\n",
        "lambda_pen = 10"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptVRgLWJd8Qg"
      },
      "source": [
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=workers)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds1t9gngeNEj"
      },
      "source": [
        "# Create the Generator and Discriminator\n",
        "netG = Generator(z_dim=z_dim, img_ch=img_ch, features_g=features_g,\n",
        "                 num_classes=num_classes, embed_size=gen_embedding,\n",
        "                 img_size = image_size).to(device)\n",
        "netD = Discriminator(img_ch=img_ch, features_d=features_d,\n",
        "                     num_classes=num_classes, img_size=image_size).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
        "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "\n",
        "netD.apply(weights_init)\n",
        "netG.apply(weights_init)\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=learning_rate, betas=(beta1, 0.9))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(beta1, 0.9))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI00TD1WfG-4",
        "outputId": "824a5012-b846-4a06-bacf-e6da66a4e2e7"
      },
      "source": [
        "fixed_noise = torch.randn(32, z_dim, 1, 1).to(device)\n",
        "os.makedirs('model/cWGAN/Generator', exist_ok=True)\n",
        "os.makedirs('model/cWGAN/Discriminator', exist_ok=True) \n",
        "# Number of training epochs\n",
        "writer_real = SummaryWriter(log_dir=f'logs_runs/logs_cWGAN/real')\n",
        "writer_fake = SummaryWriter(log_dir=f'logs_runs/logs_cWGAN/fake')\n",
        "writer_lossD = SummaryWriter(log_dir=f'logs_runs/logs_cWGAN/lossD')\n",
        "writer_lossG = SummaryWriter(log_dir=f'logs_runs/logs_cWGAN/lossG')\n",
        "writer_penalty = SummaryWriter(log_dir=f'logs_runs/logs_cWGAN/penalty')\n",
        "iters = 0\n",
        "suffix = f'lr={learning_rate}_beta={beta1}_batch={batch_size}'  \n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (real, labels) in enumerate(dataloader):\n",
        "        real = real.to(device)\n",
        "        cur_batch_size = real.shape[0]\n",
        "        labels = labels.to(device)\n",
        "        for _ in range(critic_iterations):\n",
        "            noise = torch.randn(cur_batch_size, z_dim, 1, 1).to(device)\n",
        "            fake = netG(noise, labels)\n",
        "            disc_real = netD(real, labels).reshape(-1)\n",
        "            disc_fake = netD(fake, labels).reshape(-1)\n",
        "            gp = gradient_penalty(netD, labels, real, fake, device=device)\n",
        "            errD_real = torch.mean(disc_real)\n",
        "            errD_fake = torch.mean(disc_fake)\n",
        "            loss_disc = (-(errD_real - errD_fake)\\\n",
        "                         + lambda_pen * gp)\n",
        "            netD.zero_grad()\n",
        "            loss_disc.backward(retain_graph=True)\n",
        "            optimizerD.step()\n",
        "\n",
        "        gen_fake = netD(fake, labels).view(-1)\n",
        "        loss_gen = -torch.mean(gen_fake)\n",
        "        netG.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if iters % 25 == 0:\n",
        "            print(\n",
        "                f'Epoch [{epoch}/{num_epochs}] Batch {i}/{len(dataloader)} '\n",
        "                + f'Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}'\n",
        "                )\n",
        "            with torch.no_grad():\n",
        "                fake = netG(noise, labels)\n",
        "                img_grid_real = torchvision.utils.make_grid(\n",
        "                    real[:32], normalize=True\n",
        "                )\n",
        "                img_grid_fake = torchvision.utils.make_grid(\n",
        "                    fake[:32], normalize=True\n",
        "                )\n",
        "\n",
        "                writer_real.add_image('Real', img_grid_real, global_step=iters)\n",
        "                writer_real.add_scalar('D(x)', errD_real, global_step=iters)\n",
        "                writer_fake.add_image('Fake', img_grid_fake, global_step=iters)\n",
        "                writer_fake.add_scalar('D(G(z))', errD_fake, global_step=iters)\n",
        "                writer_lossD.add_scalar('Loss_Discriminator', loss_disc.item(), global_step=iters)\n",
        "                writer_lossG.add_scalar('Loss_Generator', loss_gen.item(), global_step=iters)\n",
        "                writer_penalty.add_scalar('Gradient_Penalty', gp.item(), global_step=iters)\n",
        "        iters += 1\n",
        "torch.save(netD, 'model/cWGAN/Discriminator/' + suffix)\n",
        "torch.save(netG, 'model/cWGAN/Generator/' + suffix)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-1-1c2062fd2394>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfixed_noise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Number of training epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwriter_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf'logs_cWGAN/real'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwriter_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf'logs_cWGAN/fake'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}